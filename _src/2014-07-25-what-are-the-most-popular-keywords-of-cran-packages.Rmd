---
layout: post
title: "What are the most popular keywords of CRAN packages?"
categories:
- r
highlight:
- r
---

```{r knitsetup, echo=FALSE, results='hide', warning=FALSE, message=FALSE, cache=FALSE}
library(knitr)
opts_knit$set(base.dir='./', out.format='md')
opts_chunk$set(prompt=FALSE, comment='', results='markup',
  fig.path='/assets/images/')
```

A large proportion of R's power should be attributed to the enormous amount of extension packages. Most of the packages are hosted on [CRAN](http://cran.r-project.org).

These packages cover a wide range of fields. In this post, I'll show you how to use R to scrap the titles of all CRAN packages from the [web page](http://cran.r-project.org/web/packages/available_packages_by_date.html) and find out which keywords are the most popular.

To minimize the efforts, we try best to avoid reinventing the wheels and get some answer as quickly as possible. We only use existing packages to do all the work.

Here is our toolbox that is useful in this task:

- [`httr`](https://github.com/hadley/httr): Download and parse web pages
- [`rvest`](https://github.com/hadley/rvest): Scrape from the web page by selector
- [`rlist`](http://renkun.me/rlist): Quickly perform mapping and filtering in functional style
- [`pipeR`](http://renkun.me/pipeR): Pipe all operations at high performance

First, we equip our R environment with these tools.

```{r}
library(httr)
library(rvest)
library(rlist)
library(pipeR)
```

Then we download and parse the web page.

```{r}
url <- "http://cran.r-project.org/web/packages/available_packages_by_date.html"
html <- content(GET(url),"parsed")
```

Now `html` is a parsed HTML document object that is well structured and is ready to query. Note that we need to get the texts in the third column of the table. Here we use [XPath](https://en.wikipedia.org/wiki/XPath) to locate the information we want. Or you can use [CSS](http://www.w3.org/TR/CSS2/selector.html) selector to do the same work.

The following code are written in fluent style with pipeline.

```{r}
words <- html[xpath("//tr//td[3]//text()")] %>>%   # select the 3rd column
  list.map( # map each node to ...
    # 1. get the trimmed text in the XML node
    XML::xmlValue(.,trim = TRUE) %>>% 
      # 2. split the text by non-word-letters
      strsplit("[^a-zA-Z]") %>>% 
      # 3. put everything together in vector
      unlist(use.names = FALSE) %>>% 
      # 4. lower all words
      tolower %>>% 
      # 5. filter words with more than 3 letters to be meaningful
      list.filter(nchar(.) > 3L)) %>>% 
  # put everything in a large character vector
  unlist %>>%
  # create a table of word count
  table %>>%
  # sort the table descending
  list.sort(desc(.)) %>>%
  # take out the first 100 elements
  list.take(100) %>>%
  # print out the results
  print
```

The work is done, in 12 lines, in only a little more than 2 seconds!

If you want to know more about these packages, please visit their project pages. Hope you can do more amazing things in your work.
